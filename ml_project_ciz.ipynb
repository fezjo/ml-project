{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhg8MqHIKOd7pCT0afXa/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fezjo/ml-project/blob/master/ml_project_ciz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oXOqfWtiWh2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o8DwGNFJ42-"
      },
      "outputs": [],
      "source": [
        "from keras import layers\n",
        "\n",
        "vocab_size = 26*26\n",
        "\n",
        "# vectorize layer adapt to bigrams\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "layers.TextVectorization(\n",
        "    ngrams=(2,),\n",
        "    output_sequence_length=???,)\n",
        ")\n",
        "layers.Embedding(\n",
        "    input_dim=vocab_size,\n",
        "    output_dim=???,) \n",
        "layers.Dropout(0.25)\n",
        "3,4,5 -> layers.Conv2D(\n",
        "layers.Activation(activations.selu)\n",
        "layers.MaxPooling1D(\n",
        "layers.Dense(300, activation=activations.softmax)\n",
        "layers.Dense(authors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Use the TextVectorization layer to tokenize the input string based on bigrams\n",
        "vectorize_layer = keras.layers.TextVectorization(\n",
        "    ngrams=(2,),\n",
        "    output_sequence_length=200)\n",
        "\n",
        "# Input layer for string data\n",
        "inputs = keras.layers.Input(shape=(1,), dtype=tf.string)\n",
        "\n",
        "# Preprocessing layer\n",
        "embedding = vectorize_layer(inputs)\n",
        "\n",
        "# Add a dropout layer to prevent overfitting\n",
        "dropped = keras.layers.Dropout(0.3)(embedding)\n",
        "\n",
        "# Create 3 parallel Conv1D layers with kernel sizes of 3, 4, and 5\n",
        "conv_input = dropped\n",
        "n_conv_filters = 500\n",
        "convs = [\n",
        "    keras.layers.Conv1D(filters=n_conv_filters, kernel_size=ks, activation=\"relu\")(conv_input)\n",
        "    for ks in (3, 4, 5)\n",
        "]\n",
        "\n",
        "\n",
        "# Max pooling layer\n",
        "pooled = [keras.layers.MaxPooling1D()(conv) for conv in convs]\n",
        "\n",
        "# Flatten the output of the pooling layer\n",
        "flattened = keras.layers.Flatten()(pooled)\n",
        "\n",
        "# Concatenate the outputs of the parallel Conv1D layers\n",
        "concatenated = keras.layers.Concatenate()(flattened)\n",
        "\n",
        "# Add a dropout layer to prevent overfitting\n",
        "dropped = keras.layers.Dropout(0.3)(concatenated)\n",
        "\n",
        "# Add a dense layer\n",
        "dense = keras.layers.Dense(units=256, activation=\"relu\")(dropped)\n",
        "\n",
        "# Output layer\n",
        "outputs = keras.layers.Dense(units=1, activation=\"sigmoid\")(dense)\n",
        "\n",
        "# Create the model\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "dwVDYD6ZeWcM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}